{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(files_path, review_type):\n",
    "    try:\n",
    "        output = []\n",
    "        files = os.listdir(files_path) \n",
    "        for file in files:\n",
    "            f = open(files_path + file, 'r', encoding=\"utf8\")\n",
    "            output.append((f.read(), review_type))\n",
    "        return output\n",
    "    except IOError:\n",
    "        print('Problem opening file')\n",
    "    finally:\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_pos = get_files('../datasets/movie_reviews/data/alle/train/pos/', 0)\n",
    "train_neg = get_files('../datasets/movie_reviews/data/alle/train/neg/', 1)\n",
    "#train_pos = get_files('../datasets/movie_reviews/data/subset/train/pos/', 0)\n",
    "#train_neg = get_files('../datasets/movie_reviews/data/subset/train/neg/', 1)\n",
    "\n",
    "print('* TRAINING DATA * ')\n",
    "print('# positives reviews: ', len(train_pos))\n",
    "print('# negatives reviews', len(train_neg))\n",
    "train_data = train_pos + train_neg\n",
    "print('# total reviews: ', len(train_data))\n",
    "print('-----------------')\n",
    "# Load test data\n",
    "test_pos = get_files('../datasets/movie_reviews/data/alle/test/pos/', 0)\n",
    "test_neg = get_files('../datasets/movie_reviews/data/alle/test/neg/', 1)\n",
    "#test_pos = get_files('../datasets/movie_reviews/data/subset/test/pos/', 0)\n",
    "#test_neg = get_files('../datasets/movie_reviews/data/subset/test/neg/', 1)\n",
    "print('* TEST DATA * ')\n",
    "print('# positives reviews: ', len(test_pos))\n",
    "print('# negatives reviews', len(test_neg))\n",
    "test_data = test_pos + test_neg\n",
    "print('# total reviews: ', len(test_data))\n",
    "\n",
    "# Does not want a 50/50 split between training and test\n",
    "# Therefore creates one big set of data that later will be split into 80/20 train- and testdata\n",
    "# a = train_data[::2]\n",
    "# b = train_data[1::2]\n",
    "# c = test_data[::2]\n",
    "# d = test_data[1::2]\n",
    "# all_reviews = a + b + c + d'\n",
    "all_r = train_data + test_data\n",
    "shuffle(all_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = all_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "try:\n",
    "    f = open('../datasets/stopwords.txt', 'r')\n",
    "    stopwords = f.read().split(',')\n",
    "except IOError:\n",
    "    print('Problem opening file')\n",
    "finally:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * * * PREPROCESSING * * * \n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "preprocessed_reviews = []\n",
    "\n",
    "for t in all_reviews:\n",
    "    #print(len(preprocessed_reviews))\n",
    "    review = t[0]\n",
    "    review_type = t[1]\n",
    "    # Remove whitespace and punctutation\n",
    "    text = re.sub('[' + string.punctuation + ']', ' ', review)\n",
    "    text = re.sub('[\\n\\t\\r]', '', text)\n",
    "    \n",
    "    # Split words into list\n",
    "    words = text.split()\n",
    "    new = []\n",
    "    # Remove stopwords and stem remaining words \n",
    "    for word in words:\n",
    "        stemmed_word = stemmer.stem(word.lower())\n",
    "        if stemmed_word not in stopwords and len(stemmed_word) > 2:\n",
    "            new.append(stemmed_word)    \n",
    "    \n",
    "    # Add to preproccesed list\n",
    "    preprocessed_reviews.append((new, review_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for r in preprocessed_reviews:\n",
    "    words = r[0]\n",
    "    for w in words:\n",
    "        if w in stopwords:\n",
    "            count += 1\n",
    "a = 191569\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data in trainingdata and testdata (80-20 ratio)\n",
    "\n",
    "total = len(preprocessed_reviews) #Total number of reviews\n",
    "test_number = int(0.20 * total) # Number of testing reviews\n",
    "# Picking randomly\n",
    "print(test_number)\n",
    "copy = preprocessed_reviews[:]\n",
    "test_set = []\n",
    "\n",
    "taken = {}\n",
    "while len(test_set) < test_number:\n",
    "    #print(len(train_texts))\n",
    "    num = random.randint(0, test_number - 1)\n",
    "    if num not in taken.keys():\n",
    "        test_set.append(copy.pop(num))\n",
    "        taken[num] = 1\n",
    "\n",
    "train_set = copy[:] # Trainset is the remaining reviews\n",
    "        \n",
    "len(train_set)/total, len(test_set)/total, len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * * * TRAINING THE MODEL * * * \n",
    "\n",
    "# meaning: Computing probabilities needed for P(Positive|Word)\n",
    "\n",
    "def total_goods_and_bads(tset):\n",
    "    goods = 0\n",
    "    bads = 0\n",
    "    for t in tset:\n",
    "        goods += 1 if t[1] == 0 else 0\n",
    "        bads += 1 if t[1] == 1 else 0\n",
    "    return goods, bads\n",
    "\n",
    "total_positive = total_goods_and_bads(train_set)[0]\n",
    "total_negative = total_goods_and_bads(train_set)[1]\n",
    "print(total_positive)\n",
    "print(total_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First making a word counter for pos and neg reviews\n",
    "pos_word_counter = {}\n",
    "neg_word_counter = {}\n",
    "total_words = 0\n",
    "for t in train_set:\n",
    "    review = t[0]\n",
    "    review_type = t[1]\n",
    "    already_counted = []\n",
    "    for word in review:\n",
    "        total_words += 1\n",
    "        if review_type == 0:\n",
    "            if word not in pos_word_counter:\n",
    "                pos_word_counter[word] = 1\n",
    "            else:\n",
    "                if word not in already_counted:\n",
    "                    pos_word_counter[word] += 1  \n",
    "        else:\n",
    "            if word not in neg_word_counter:\n",
    "                neg_word_counter[word] = 1\n",
    "            else:\n",
    "                if word not in already_counted:\n",
    "                    neg_word_counter[word] += 1\n",
    "                    \n",
    "        already_counted.append(word)\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes words that are not inluded in at least 0.15% of the reviews\n",
    "removed_words = 0\n",
    "for j in range(len(train_set)):\n",
    "    words = train_set[j][0]\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        word = words[i]\n",
    "        word_removed = False\n",
    "        if word in pos_word_counter:\n",
    "            if pos_word_counter[word] < 0.0015*len(train_set):\n",
    "                train_set[j][0].remove(word)\n",
    "                word_removed = True\n",
    "                removed_words += 1\n",
    "        elif word in neg_word_counter:\n",
    "            if neg_word_counter[word] < 0.0015*len(train_set):\n",
    "                train_set[j][0].remove(word)\n",
    "                word_removed = True\n",
    "                removed_words += 1\n",
    "        if not word_removed:\n",
    "            i += 1\n",
    "    j += 1\n",
    "removed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(dicti, end):\n",
    "    # Sorterer etter value i dict, gir liste med tupler\n",
    "    most_common_words = sorted(dicti.items(), key = lambda kv: kv[1])\n",
    "    most_common_words.reverse()\n",
    "    most_common_words = most_common_words[:end]\n",
    "    # Lager dict på formen {word: count, ...}\n",
    "    # Vil ha dict fremfor liste med tupler, pga. senere søk\n",
    "    return dict(most_common_words)                \n",
    "\n",
    "most_used_words_pos = sort_dict(pos_word_counter, 25)\n",
    "most_used_words_neg = sort_dict(neg_word_counter, 25)\n",
    "most_used_words_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Need these 4 probabilities\n",
    "# 1) Probability that a word appears in positive reviews\n",
    "# 2) Probability that a word appears in negative reviews\n",
    "# 3) Overall probability that any given review is positive\n",
    "# 4) Overall probability that any given reviews is negative\n",
    "\n",
    "# # Making a dictionary with probabilities for different words appearing in good and bad reviews\n",
    "# # Example: {'bad': (0.0881, 0.3226)}\n",
    "probability_appearing = {}\n",
    "for t in train_set:\n",
    "    text = t[0]\n",
    "    for word in text:\n",
    "        if word not in probability_appearing:\n",
    "            if word in pos_word_counter:\n",
    "                p_appearing_good = pos_word_counter[word]/total_positive\n",
    "            else:\n",
    "                p_appearing_good = 0.1\n",
    "            if word in neg_word_counter:\n",
    "                p_appearing_bad = neg_word_counter[word]/total_negative\n",
    "            else:\n",
    "                p_appearing_bad = 0.1\n",
    "            probability_appearing[word] = (p_appearing_good, p_appearing_bad)\n",
    "            \n",
    "\n",
    "p_pos = total_positive/len(train_set)\n",
    "p_neg = total_negative/len(train_set)\n",
    "print(p_good)\n",
    "print(p_bad)\n",
    "\n",
    "\n",
    "# Finally we can compute P(Positive | Word)\n",
    "def p_is_positive_given_word(word):\n",
    "    return (probability_appearing[word][0]*p_pos)/((probability_appearing[word][0]*p_pos + probability_appearing[word][1]*p_neg))\n",
    "\n",
    "def p_is_negative_given_word(word):\n",
    "    return (probability_appearing[word][1]*p_neg)/((probability_appearing[word][1]*p_neg + probability_appearing[word][0]*p_pos))\n",
    "\n",
    "p_is_positive_given_word('bad'), p_is_negative_given_word('bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = {}\n",
    "for t in train_set:\n",
    "    text = t[0]\n",
    "    for word in text:\n",
    "        if word not in probabilities:\n",
    "            p_pos = p_is_positive_given_word(word)\n",
    "            p_neg = p_is_negative_given_word(word)\n",
    "            if p_pos == 0:\n",
    "                p_pos = 0.1 # tweaking this value\n",
    "            if p_pos == 1:\n",
    "                p_pos = 0.98\n",
    "            if p_neg == 0:\n",
    "                p_neg = 0.1\n",
    "            if p_neg == 1:\n",
    "                p_neg = 0.98\n",
    "                \n",
    "            probabilities[word] = (p_pos, p_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out words that are not informative (probabilities between 0.45 and 0.55)\n",
    "print(len(probabilities))\n",
    "for word in list(probabilities):\n",
    "    probs = probabilities[word]\n",
    "    if 0.40 < probs[0] and probs[0] < 0.60 and 0.40 < probs[1] and probs[1] < 0.60:\n",
    "        del probabilities[word]\n",
    "print(len(probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINING INDIVIDUAL PROBABILITIES\n",
    "# Determining whether a message is spam or ham based only on the presence of one word is error-prone,\n",
    "# must try to consider all the words (or the most interesting) in the message\n",
    "\n",
    "from functools import reduce\n",
    "def p_is_type(words):\n",
    "    words = list(filter(lambda x: x in probabilities, words)) # Filter out words not met during training-fase\n",
    "    pos_probs = []\n",
    "    neg_probs = []\n",
    "    for word in words:\n",
    "        pos_probs.append(probabilities[word][0])\n",
    "        neg_probs.append(probabilities[word][1])\n",
    "        #else:\n",
    "         #   probs.append(0.5) # tweaking this value\n",
    "    pos_probs_not = list(map(lambda prob: 1-prob, pos_probs))\n",
    "    neg_probs_not = list(map(lambda prob: 1-prob, neg_probs))\n",
    "    \n",
    "    pos_product = reduce(lambda x, y: x * y, pos_probs, 1)\n",
    "    neg_product = reduce(lambda x, y: x * y, neg_probs, 1) \n",
    "    \n",
    "    pos_product_not = reduce(lambda x, y: x * y, pos_probs_not, 1)\n",
    "    neg_product_not = reduce(lambda x, y: x * y, neg_probs_not, 1)\n",
    "    return pos_product/(pos_product + pos_product_not), neg_product/(neg_product + neg_product_not) \n",
    "\n",
    "p_is_type(['good', 'enjoy', 'well']), p_is_type(['terribl', 'hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * * * TESTING THE MODEL * * * \n",
    "total_correct = 0\n",
    "\n",
    "true_good_as_good = 0\n",
    "true_good_as_bad = 0\n",
    "\n",
    "true_bad_as_bad = 0\n",
    "true_bad_as_good = 0\n",
    "count = 0\n",
    "for t in test_set:\n",
    "    guess = -1\n",
    "    words = t[0]\n",
    "    answer = t[1]\n",
    "    try:\n",
    "        p_positive = p_is_type(words)[0]\n",
    "        p_negative = p_is_type(words)[1]\n",
    "    except:\n",
    "        count += 1\n",
    "        #print(words)\n",
    "\n",
    "    guess = 0 if p_positive > p_negative else 1\n",
    "    if guess == answer:\n",
    "        total_correct += 1\n",
    "        if answer == 0: # true negative\n",
    "            true_good_as_good += 1\n",
    "        else: # true positive\n",
    "            true_bad_as_bad += 1 \n",
    "    else:\n",
    "        #print(words, answer)\n",
    "        if answer == 0: # false positive\n",
    "            true_good_as_bad += 1\n",
    "        else: # true negative\n",
    "            true_bad_as_good += 1\n",
    "\n",
    "            \n",
    "true_positives = total_goods_and_bads(test_set)[0]\n",
    "true_negatives = total_goods_and_bads(test_set)[1]\n",
    "\n",
    "print('Total test texts: ', len(test_set))\n",
    "print('Number of correct: ', total_correct)\n",
    "print('Accuracy: ', total_correct*100/(true_positives+true_negatives))\n",
    "print('-------------------------------')\n",
    "print('Positives precision: ', true_good_as_good/(true_good_as_good + true_bad_as_good))\n",
    "print('Positives recall: ', true_good_as_good/(true_good_as_good + true_good_as_bad))\n",
    "print('Negatives precision: ', true_bad_as_bad/(true_bad_as_bad + true_good_as_bad))\n",
    "print('Negatives recall: ', true_bad_as_bad/(true_bad_as_bad + true_bad_as_good))\n",
    "print('-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * * * VISUALISATIONS * * * \n",
    "from wordcloud import WordCloud\n",
    "\n",
    "pos_reviews = \"\"\n",
    "neg_reviews = \"\"\n",
    "revs = all_reviews[:100]\n",
    "\n",
    "for t in revs:\n",
    "    review = t[0].split()\n",
    "    s = \"\"\n",
    "    for word in review:\n",
    "        if len(word) > 2:\n",
    "            s += word + ' '\n",
    "    text = re.sub('[' + string.punctuation + ']', ' ', s)\n",
    "    text = re.sub('[\\n\\t\\r]', '', text)\n",
    "    if t[1] == 0:\n",
    "        pos_reviews += text\n",
    "    else:\n",
    "        neg_reviews += text\n",
    "\n",
    "# Generate a word cloud image\n",
    "pos_wordcloud = WordCloud(width=600, height=400).generate(pos_reviews)\n",
    "#neg_wordcloud = WordCloud(width=600, height=400).generate(neg_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spam Word cloud\n",
    "plt.figure(figsize=(10,8), facecolor='k')\n",
    "plt.imshow(pos_wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
